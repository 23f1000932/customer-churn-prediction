{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":107399,"databundleVersionId":13009703,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:50.399355Z","iopub.execute_input":"2025-07-23T07:40:50.399698Z","iopub.status.idle":"2025-07-23T07:40:50.796791Z","shell.execute_reply.started":"2025-07-23T07:40:50.399672Z","shell.execute_reply":"2025-07-23T07:40:50.795825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:50.798311Z","iopub.execute_input":"2025-07-23T07:40:50.798795Z","iopub.status.idle":"2025-07-23T07:40:51.959565Z","shell.execute_reply.started":"2025-07-23T07:40:50.798769Z","shell.execute_reply":"2025-07-23T07:40:51.958356Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data types of different columns**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/mlp-term-2-2025-kaggle-assignment-2/train.csv\")\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:51.960638Z","iopub.execute_input":"2025-07-23T07:40:51.961216Z","iopub.status.idle":"2025-07-23T07:40:52.290915Z","shell.execute_reply.started":"2025-07-23T07:40:51.961183Z","shell.execute_reply":"2025-07-23T07:40:52.289806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Descriptive statistics of numerical columns**","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:52.291982Z","iopub.execute_input":"2025-07-23T07:40:52.292321Z","iopub.status.idle":"2025-07-23T07:40:52.393583Z","shell.execute_reply.started":"2025-07-23T07:40:52.292292Z","shell.execute_reply":"2025-07-23T07:40:52.392318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:52.396184Z","iopub.execute_input":"2025-07-23T07:40:52.397355Z","iopub.status.idle":"2025-07-23T07:40:52.404441Z","shell.execute_reply.started":"2025-07-23T07:40:52.397324Z","shell.execute_reply":"2025-07-23T07:40:52.403169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Visualizing Data**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\n\n# Suppress future warning from seaborn/pandas\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*use_inf_as_na.*\")\n\n# Drop columns not needed\ndf_viz = df.drop(columns=['id', 'customer_id','exit_status','last_name'])\n\n# Set seaborn style\nsns.set(style=\"whitegrid\")\n\n# Plot each column\nfor col in df_viz.columns:\n    plt.figure(figsize=(10, 5))\n    \n    if df_viz[col].dtype == 'object':\n        sns.countplot(data=df_viz, x=col, order=df_viz[col].value_counts().index[:10])\n        plt.title(f\"Count Plot of '{col}'\")\n        plt.xticks(rotation=45)\n    else:\n        sns.histplot(data=df_viz, x=col, kde=True, bins=30)\n        plt.title(f\"Distribution of '{col}'\")\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:52.405560Z","iopub.execute_input":"2025-07-23T07:40:52.405916Z","iopub.status.idle":"2025-07-23T07:40:59.492963Z","shell.execute_reply.started":"2025-07-23T07:40:52.405886Z","shell.execute_reply":"2025-07-23T07:40:59.491993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Identify and handle duplicates**","metadata":{}},{"cell_type":"code","source":"subset_cols = ['last_name', 'credit_score', 'country', 'gender',\n       'age', 'tenure', 'acc_balance', 'prod_count', 'has_card', 'is_active',\n       'estimated_salary']\n\n# Identify duplicates\nduplicates = df[df[subset_cols].duplicated()]\nprint(f\"Number of duplicate rows before droping: {len(duplicates)}\")\n\n# Drop duplicates (keep the first occurrence)\ndf = df.drop_duplicates(subset=subset_cols, keep='first').reset_index(drop=True)\n\nduplicates = df[df[subset_cols].duplicated()]\nprint(f\"Number of duplicate rows after droping: {len(duplicates)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.493911Z","iopub.execute_input":"2025-07-23T07:40:59.494296Z","iopub.status.idle":"2025-07-23T07:40:59.681471Z","shell.execute_reply.started":"2025-07-23T07:40:59.494275Z","shell.execute_reply":"2025-07-23T07:40:59.680454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Handling Null Values**","metadata":{}},{"cell_type":"code","source":"null_counts = df.isnull().sum(axis=1)\n\n# Identify rows with more than 2 NaNs\nrows_with_many_nulls = df[null_counts > 2]\nprint(f\"Number of rows with more than 2 nulls before droping: {len(rows_with_many_nulls)}\")\n\n# Drop those rows\ndf = df[null_counts <= 2].reset_index(drop=True)\n\n\nnull_counts = df.isnull().sum(axis=1)\n\n# Identify rows with more than 2 NaNs\nrows_with_many_nulls = df[null_counts > 2]\nprint(f\"Number of rows with more than 2 nulls after droping: {len(rows_with_many_nulls)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.682458Z","iopub.execute_input":"2025-07-23T07:40:59.682780Z","iopub.status.idle":"2025-07-23T07:40:59.771135Z","shell.execute_reply.started":"2025-07-23T07:40:59.682721Z","shell.execute_reply":"2025-07-23T07:40:59.770202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Handling outliers**","metadata":{}},{"cell_type":"code","source":"target_cols = ['credit_score',\n       'age', 'tenure', 'acc_balance', 'prod_count',\n       'estimated_salary'] \ndf_cleaned = df.copy()\n\nfor col in target_cols:\n    # Calculate Q1, Q3 only from non-NaN values\n    Q1 = df_cleaned[col].quantile(0.25)\n    Q3 = df_cleaned[col].quantile(0.75)\n    IQR = Q3 - Q1\n\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Filter out only valid (non-NaN) rows that are within bounds, or keep NaNs as-is\n    with np.errstate(invalid='ignore'):\n        condition = (df_cleaned[col].isna()) | ((df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound))\n    outliers = (~condition).sum()\n\n    print(f\"{col}: {outliers} outliers removed\")\n    df_cleaned = df_cleaned[condition]\n\ndf_cleaned.reset_index(drop=True, inplace=True)\nprint(\"Final shape after removing outliers:\", df_cleaned.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.771963Z","iopub.execute_input":"2025-07-23T07:40:59.772218Z","iopub.status.idle":"2025-07-23T07:40:59.847078Z","shell.execute_reply.started":"2025-07-23T07:40:59.772198Z","shell.execute_reply":"2025-07-23T07:40:59.846097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df_cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.848016Z","iopub.execute_input":"2025-07-23T07:40:59.848313Z","iopub.status.idle":"2025-07-23T07:40:59.855346Z","shell.execute_reply.started":"2025-07-23T07:40:59.848287Z","shell.execute_reply":"2025-07-23T07:40:59.854257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Dropping unnecessary columns**","metadata":{}},{"cell_type":"code","source":"df.drop(columns=['id', 'customer_id','last_name'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.856181Z","iopub.execute_input":"2025-07-23T07:40:59.856429Z","iopub.status.idle":"2025-07-23T07:40:59.877892Z","shell.execute_reply.started":"2025-07-23T07:40:59.856411Z","shell.execute_reply":"2025-07-23T07:40:59.877064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Train Test split**","metadata":{}},{"cell_type":"code","source":"y = df.pop('exit_status')\nx_train , x_val , y_train , y_val = train_test_split(df,y,random_state=39,test_size=0.2)\nprint(\"x_train shape:\", x_train.shape)\nprint(\"x_val shape:\", x_val.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_val shape:\", y_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.878931Z","iopub.execute_input":"2025-07-23T07:40:59.879250Z","iopub.status.idle":"2025-07-23T07:40:59.901539Z","shell.execute_reply.started":"2025-07-23T07:40:59.879220Z","shell.execute_reply":"2025-07-23T07:40:59.900705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Imputing missing values**","metadata":{}},{"cell_type":"code","source":"list_tran_1 = []\ncount = 0\nfor i in list(x_train.columns):\n    if  x_train[i].dtype == 'O':\n        list_tran_1.append((\"v{}\".format(count),SimpleImputer(strategy=\"most_frequent\") ,[i]))\n    else:\n        list_tran_1.append((\"v{}\".format(count),SimpleImputer(strategy=\"mean\"),[i]))\n    count += 1\nprint(list_tran_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.902432Z","iopub.execute_input":"2025-07-23T07:40:59.902687Z","iopub.status.idle":"2025-07-23T07:40:59.918142Z","shell.execute_reply.started":"2025-07-23T07:40:59.902667Z","shell.execute_reply":"2025-07-23T07:40:59.917112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tran_1 = ColumnTransformer(list_tran_1)\ntran_1.set_output(transform=\"pandas\")\nx_train_tran_1 = tran_1.fit_transform(x_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:40:59.920956Z","iopub.execute_input":"2025-07-23T07:40:59.921241Z","iopub.status.idle":"2025-07-23T07:41:00.011575Z","shell.execute_reply.started":"2025-07-23T07:40:59.921218Z","shell.execute_reply":"2025-07-23T07:41:00.010543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Encoding Categorical Columns and Scaling Numerical Columns**","metadata":{}},{"cell_type":"code","source":"print(x_train_tran_1.columns)\nx_train_tran_1.head()\ncat_cols = x_train_tran_1.select_dtypes(include=['object']).columns\n\n# Print unique values for each categorical column\nprint(\"\\nUnique values in categorical columns:\")\nfor col in cat_cols:\n    print(f\"\\n{col}:\")\n    print(x_train_tran_1[col].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:41:00.012816Z","iopub.execute_input":"2025-07-23T07:41:00.013101Z","iopub.status.idle":"2025-07-23T07:41:00.030986Z","shell.execute_reply.started":"2025-07-23T07:41:00.013081Z","shell.execute_reply":"2025-07-23T07:41:00.030050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ordinal_cols = ['v2__gender']\nordinal_values = [\n    ['Male' ,'Female']   \n]\n\nnominal_cols = ['v1__country']\n\n# Identify numerical columns\nnum_cols = ['v0__credit_score', 'v3__age',\n       'v4__tenure', 'v5__acc_balance', 'v6__prod_count', 'v7__has_card',\n       'v8__is_active', 'v9__estimated_salary']\n\n# ColumnTransformer setup\ntran_2 = ColumnTransformer(transformers=[\n    ('ordinal', OrdinalEncoder(categories=ordinal_values), ordinal_cols),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'), nominal_cols),\n    ('scaler', StandardScaler(), num_cols)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:41:00.031874Z","iopub.execute_input":"2025-07-23T07:41:00.032174Z","iopub.status.idle":"2025-07-23T07:41:00.039389Z","shell.execute_reply.started":"2025-07-23T07:41:00.032133Z","shell.execute_reply":"2025-07-23T07:41:00.037851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train_tran_2 = tran_2.fit_transform(x_train_tran_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:41:00.041084Z","iopub.execute_input":"2025-07-23T07:41:00.041449Z","iopub.status.idle":"2025-07-23T07:41:00.145017Z","shell.execute_reply.started":"2025-07-23T07:41:00.041426Z","shell.execute_reply":"2025-07-23T07:41:00.143614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_val_tran_1 = tran_1.transform(x_val)\nx_val_tran_2 = tran_2.transform(x_val_tran_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:41:00.145682Z","iopub.execute_input":"2025-07-23T07:41:00.145957Z","iopub.status.idle":"2025-07-23T07:41:00.193917Z","shell.execute_reply.started":"2025-07-23T07:41:00.145937Z","shell.execute_reply":"2025-07-23T07:41:00.192991Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Traning 7 Different models**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report\n)\nimport pandas as pd\nimport numpy as np\n\n# --- Models (7) ---\ncls_models = {\n    'Logistic': LogisticRegression(random_state=39, max_iter=1000),\n    'XGBoost':  XGBClassifier(random_state=39, eval_metric='logloss', use_label_encoder=False),\n    'DecisionTree': DecisionTreeClassifier(random_state=39),\n    'RandomForest': RandomForestClassifier(random_state=39),\n    'GradientBoosting': GradientBoostingClassifier(random_state=39),\n    'KNN': KNeighborsClassifier(),\n    'SVM': SVC(random_state=39, probability=True)  # enable predict_proba for ROC-AUC\n}\n\ncls_results = []\n\n# Helper: safely compute ROC-AUC (binary vs multi-class)\ndef safe_roc_auc(y_true, y_proba):\n    # y_proba: array of shape (n_samples, n_classes) or (n_samples,) for binary\n    try:\n        if y_proba.ndim == 1 or y_proba.shape[1] == 1:  # binary probs or decision scores\n            return roc_auc_score(y_true, y_proba)\n        else:\n            # multi-class: one-vs-rest macro average\n            return roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n    except Exception:\n        return np.nan\n\nfor name, model in cls_models.items():\n    # Fit\n    model.fit(x_train_tran_2, y_train)\n\n    # Predict class labels\n    y_pred_train = model.predict(x_train_tran_2)\n    y_pred_val   = model.predict(x_val_tran_2)\n\n    # Predict probabilities or decision scores for ROC-AUC\n    if hasattr(model, \"predict_proba\"):\n        proba_train = model.predict_proba(x_train_tran_2)\n        proba_val   = model.predict_proba(x_val_tran_2)\n        # for binary, roc_auc_score expects prob of positive class\n        if proba_train.shape[1] == 2:\n            proba_train = proba_train[:, 1]\n            proba_val   = proba_val[:, 1]\n    elif hasattr(model, \"decision_function\"):\n        proba_train = model.decision_function(x_train_tran_2)\n        proba_val   = model.decision_function(x_val_tran_2)\n    else:\n        proba_train = np.full(len(y_train), np.nan)\n        proba_val   = np.full(len(y_val), np.nan)\n\n    # Metrics (macro average handles class imbalance better than micro for precision/recall/F1)\n    acc_train = accuracy_score(y_train, y_pred_train)\n    prec_train = precision_score(y_train, y_pred_train, average='macro', zero_division=0)\n    rec_train  = recall_score(y_train, y_pred_train, average='macro', zero_division=0)\n    f1_train   = f1_score(y_train, y_pred_train, average='macro', zero_division=0)\n    auc_train  = safe_roc_auc(y_train, proba_train)\n\n    acc_val = accuracy_score(y_val, y_pred_val)\n    prec_val = precision_score(y_val, y_pred_val, average='macro', zero_division=0)\n    rec_val  = recall_score(y_val, y_pred_val, average='macro', zero_division=0)\n    f1_val   = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\n    auc_val  = safe_roc_auc(y_val ,proba_val)\n\n    cls_results.append({\n        'Model': name,\n        'Acc_Train': acc_train, 'Prec_Train': prec_train, 'Rec_Train': rec_train, 'F1_Train': f1_train, 'AUC_Train': auc_train,\n        'Acc_Val': acc_val, 'Prec_Val': prec_val, 'Rec_Val': rec_val, 'F1_Val': f1_val, 'AUC_Val': auc_val\n    })\n\n# Results table\ncls_results_df = pd.DataFrame(cls_results).sort_values(by='Acc_Val', ascending=False)\nprint(cls_results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:41:00.194889Z","iopub.execute_input":"2025-07-23T07:41:00.195204Z","iopub.status.idle":"2025-07-23T08:08:08.138404Z","shell.execute_reply.started":"2025-07-23T07:41:00.195176Z","shell.execute_reply":"2025-07-23T08:08:08.136595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Comparision between 7 models**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Assuming 'cls_results_df' from your classification metrics (already created)\n# cls_results_df = pd.DataFrame(cls_results).sort_values(by='Acc_Val', ascending=False)\ncls_results_df.set_index('Model', inplace=True)\n\n# Common parameters\nbar_width = 0.35\nindex = np.arange(len(cls_results_df))\n\n# -------- Accuracy Plot --------\nplt.figure(figsize=(10, 6))\nbar1 = plt.bar(index, cls_results_df['Acc_Train'], bar_width, label='Train Accuracy', color='skyblue')\nbar2 = plt.bar(index + bar_width, cls_results_df['Acc_Val'], bar_width, label='Validation Accuracy', color='navy')\n\nplt.xticks(index + bar_width / 2, cls_results_df.index, rotation=45)\nplt.title('Accuracy: Train vs Validation')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1.05)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nfor bar in bar1 + bar2:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f\"{height:.2f}\", ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# -------- Precision Plot --------\nplt.figure(figsize=(10, 6))\nbar1 = plt.bar(index, cls_results_df['Prec_Train'], bar_width, label='Train Precision', color='lightcoral')\nbar2 = plt.bar(index + bar_width, cls_results_df['Prec_Val'], bar_width, label='Validation Precision', color='darkred')\n\nplt.xticks(index + bar_width / 2, cls_results_df.index, rotation=45)\nplt.title('Precision: Train vs Validation')\nplt.ylabel('Precision')\nplt.ylim(0, 1.05)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nfor bar in bar1 + bar2:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f\"{height:.2f}\", ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# -------- Recall Plot --------\nplt.figure(figsize=(10, 6))\nbar1 = plt.bar(index, cls_results_df['Rec_Train'], bar_width, label='Train Recall', color='lightgreen')\nbar2 = plt.bar(index + bar_width, cls_results_df['Rec_Val'], bar_width, label='Validation Recall', color='green')\n\nplt.xticks(index + bar_width / 2, cls_results_df.index, rotation=45)\nplt.title('Recall: Train vs Validation')\nplt.ylabel('Recall')\nplt.ylim(0, 1.05)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nfor bar in bar1 + bar2:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f\"{height:.2f}\", ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# -------- F1 Score Plot --------\nplt.figure(figsize=(10, 6))\nbar1 = plt.bar(index, cls_results_df['F1_Train'], bar_width, label='Train F1', color='orchid')\nbar2 = plt.bar(index + bar_width, cls_results_df['F1_Val'], bar_width, label='Validation F1', color='purple')\n\nplt.xticks(index + bar_width / 2, cls_results_df.index, rotation=45)\nplt.title('F1 Score: Train vs Validation')\nplt.ylabel('F1 Score')\nplt.ylim(0, 1.05)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nfor bar in bar1 + bar2:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f\"{height:.2f}\", ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# -------- AUC Plot --------\nplt.figure(figsize=(10, 6))\nbar1 = plt.bar(index, cls_results_df['AUC_Train'], bar_width, label='Train AUC', color='gold')\nbar2 = plt.bar(index + bar_width, cls_results_df['AUC_Val'], bar_width, label='Validation AUC', color='darkgoldenrod')\n\nplt.xticks(index + bar_width / 2, cls_results_df.index, rotation=45)\nplt.title('ROC-AUC: Train vs Validation')\nplt.ylabel('AUC')\nplt.ylim(0, 1.05)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nfor bar in bar1 + bar2:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f\"{height:.2f}\", ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T08:08:08.139438Z","iopub.execute_input":"2025-07-23T08:08:08.139935Z","iopub.status.idle":"2025-07-23T08:08:10.017858Z","shell.execute_reply.started":"2025-07-23T08:08:08.139908Z","shell.execute_reply":"2025-07-23T08:08:10.016926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Hyper Parameter Tuning of XGBClassifier , GradientBoostingClassifier , RandomForestClassifier finding Best model with best hyper parameters**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, make_scorer\n\n# F1 scorer (for binary or multi-class)\nf1_scorer = make_scorer(f1_score, average='weighted')\n\n# XGBoost Classifier\nxgb_params = {\n    'n_estimators': [200,300],\n    'max_depth': [5,7],\n    'learning_rate': [0.1,0.5],\n    'subsample': [0.8,1],\n    'colsample_bytree': [0.8,1],\n    'reg_lambda': [1,0.1]\n    \n}\nxgb = XGBClassifier(random_state=39, use_label_encoder=False, eval_metric='logloss')\ngrid_xgb = GridSearchCV(xgb, xgb_params, cv=5, scoring=f1_scorer, n_jobs=-1, verbose=1)\ngrid_xgb.fit(x_train_tran_2, y_train)\nxgb_best = grid_xgb.best_estimator_\n\n# Gradient Boosting Classifier\ngb_params = {\n    'n_estimators': [200,300],\n    'max_depth': [5, 7],\n    'learning_rate': [0.1]\n}\ngb = GradientBoostingClassifier(random_state=39)\ngrid_gb = GridSearchCV(gb, gb_params, cv=5, scoring=f1_scorer, n_jobs=-1, verbose=1)\ngrid_gb.fit(x_train_tran_2, y_train)\ngb_best = grid_gb.best_estimator_\n\n# Random Forest Classifier\nrf_params = {\n    'n_estimators': [100, 200],\n    'max_depth': [5,7]\n}\nrf = RandomForestClassifier(random_state=39)\ngrid_rf = GridSearchCV(rf, rf_params, cv=5, scoring=f1_scorer, n_jobs=-1, verbose=1)\ngrid_rf.fit(x_train_tran_2, y_train)\nrf_best = grid_rf.best_estimator_\n\n# Compare on Validation Set\nmodels = {\n    'XGBoost': xgb_best,\n    'GradientBoosting': gb_best,\n    'RandomForest': rf_best\n}\n\nprint(\"\\nModel Comparison on Validation Set:\")\nfor name, model in models.items():\n    y_pred = model.predict(x_val_tran_2)\n    f1 = f1_score(y_val, y_pred, average='weighted')\n    print(f\"{name:17s} | F1 Score: {f1:.4f}\")\n\n# Best model by F1\nbest_model_name = max(models, key=lambda m: f1_score(y_val, models[m].predict(x_val_tran_2), average='weighted'))\nprint(f\"\\nBest Model: {best_model_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T08:08:10.019294Z","iopub.execute_input":"2025-07-23T08:08:10.019595Z","iopub.status.idle":"2025-07-23T08:19:38.546244Z","shell.execute_reply.started":"2025-07-23T08:08:10.019570Z","shell.execute_reply":"2025-07-23T08:19:38.545198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"XGBoost Best Params:\", grid_xgb.best_params_)\nprint(\"Gradient Boosting Best Params:\", grid_gb.best_params_)\nprint(\"Random Forest Best Params:\", grid_rf.best_params_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T08:19:38.547190Z","iopub.execute_input":"2025-07-23T08:19:38.547477Z","iopub.status.idle":"2025-07-23T08:19:38.553082Z","shell.execute_reply.started":"2025-07-23T08:19:38.547454Z","shell.execute_reply":"2025-07-23T08:19:38.552064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Final Model**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\n\n# --- Train XGBoost Classifier ---\nmodel = XGBClassifier(\n    colsample_bytree=1,\n    learning_rate=0.1,\n    max_depth=5,\n    n_estimators=200,\n    reg_lambda=0.1,\n    subsample=0.8,\n    random_state=39,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\nmodel.fit(x_train_tran_2, y_train)\n\n# --- Predictions ---\ny_pred_train = model.predict(x_train_tran_2)\ny_pred_test  = model.predict(x_val_tran_2)\n\n# --- F1 Score ---\nf1_train = f1_score(y_train, y_pred_train)\nf1_test  = f1_score(y_val, y_pred_test)\n\nprint(f\"XGBoost F1 Score (Train): {f1_train:.4f}\")\nprint(f\"XGBoost F1 Score (Test) : {f1_test:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T08:19:38.554010Z","iopub.execute_input":"2025-07-23T08:19:38.554338Z","iopub.status.idle":"2025-07-23T08:19:39.474927Z","shell.execute_reply.started":"2025-07-23T08:19:38.554312Z","shell.execute_reply":"2025-07-23T08:19:39.473899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"x_test = pd.read_csv(\"/kaggle/input/mlp-term-2-2025-kaggle-assignment-2/test.csv\")\n\nid_col = x_test['id']\nx_test.drop(columns=['id', 'customer_id','last_name'], inplace=True)\nx_test_tran_1 = tran_1.transform(x_test)\nx_test_tran_2 = tran_2.transform(x_test_tran_1)\n\ny_pred = model.predict(x_test_tran_2)\n\nsubmission = pd.DataFrame({\n    'id': id_col,\n    'exit_status': y_pred\n})\n\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T08:19:39.476187Z","iopub.execute_input":"2025-07-23T08:19:39.476419Z","iopub.status.idle":"2025-07-23T08:19:39.689445Z","shell.execute_reply.started":"2025-07-23T08:19:39.476402Z","shell.execute_reply":"2025-07-23T08:19:39.687795Z"}},"outputs":[],"execution_count":null}]}